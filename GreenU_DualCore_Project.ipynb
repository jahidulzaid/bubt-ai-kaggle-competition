{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e25699",
   "metadata": {},
   "source": [
    "# GreenU DualCore Project - Multi-Class Classification\n",
    "\n",
    "Cross-validated XGBoost ensemble for predicting Status (C, CL, D) with optimal preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8908d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcec7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def4a7a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e021741",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(f\"Target distribution:\\n{train_df['Status'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e2179",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "Handles numeric (median impute + scaling) and categorical (mode impute + one-hot encoding) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(cat_cols, num_cols):\n",
    "    \"\"\"Create preprocessing pipeline for numeric and categorical fields.\"\"\"\n",
    "    return ColumnTransformer(\n",
    "        [\n",
    "            (\n",
    "                \"num\",\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                    ]\n",
    "                ),\n",
    "                num_cols,\n",
    "            ),\n",
    "            (\n",
    "                \"cat\",\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "                    ]\n",
    "                ),\n",
    "                cat_cols,\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81302624",
   "metadata": {},
   "source": [
    "## Cross-Validated Training\n",
    "\n",
    "5-fold stratified CV with XGBoost, averaging predictions across folds for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_ensemble(train_df, test_df, params, n_splits=5, random_state=42):\n",
    "    \"\"\"Cross-validated training with fold-level models averaged for robustness.\"\"\"\n",
    "    y_raw = train_df[\"Status\"]\n",
    "    X = train_df.drop(columns=[\"Status\", \"id\"])\n",
    "    X_test_raw = test_df.drop(columns=[\"id\"])\n",
    "\n",
    "    cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    oof = np.zeros((len(train_df), len(label_encoder.classes_)))\n",
    "    test_pred = np.zeros((len(test_df), len(label_encoder.classes_)))\n",
    "    fold_best_rounds = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "        preprocessor = build_preprocessor(cat_cols, num_cols)\n",
    "        X_tr_proc = preprocessor.fit_transform(X_tr)\n",
    "        X_va_proc = preprocessor.transform(X_va)\n",
    "        X_test_proc = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_tr_proc, y_tr, eval_set=[(X_va_proc, y_va)], verbose=False)\n",
    "\n",
    "        va_pred = model.predict_proba(X_va_proc)\n",
    "        oof[va_idx] = va_pred\n",
    "        fold_loss = log_loss(y_va, va_pred, labels=[0, 1, 2])\n",
    "        best_round = (\n",
    "            model.best_iteration + 1\n",
    "            if getattr(model, \"best_iteration\", None) is not None\n",
    "            else params[\"n_estimators\"]\n",
    "        )\n",
    "        fold_best_rounds.append(best_round)\n",
    "        print(f\"Fold {fold}: logloss={fold_loss:.4f}, rounds={best_round}\")\n",
    "\n",
    "        test_pred += model.predict_proba(X_test_proc)\n",
    "\n",
    "    test_pred /= n_splits\n",
    "    cv_loss = log_loss(y, oof, labels=[0, 1, 2])\n",
    "    avg_rounds = int(np.round(np.mean(fold_best_rounds)))\n",
    "    print(f\"CV logloss={cv_loss:.4f}, avg best rounds={avg_rounds}\")\n",
    "\n",
    "    return oof, test_pred, label_encoder, avg_rounds, cv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc4de0",
   "metadata": {},
   "source": [
    "## Configure XGBoost Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda37772",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = dict(\n",
    "    n_estimators=1200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=0.8,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    objective=\"multi:softprob\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    num_class=3,\n",
    "    tree_method=\"hist\",\n",
    "    reg_lambda=1.2,\n",
    "    reg_alpha=0.15,\n",
    "    gamma=0.05,\n",
    "    early_stopping_rounds=80,\n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc854dc",
   "metadata": {},
   "source": [
    "## Run Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof, test_pred, label_encoder, avg_rounds, cv_loss = train_cv_ensemble(\n",
    "    train_df, test_df, base_params, n_splits=5, random_state=99\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ca45a",
   "metadata": {},
   "source": [
    "## Final Model on Full Data\n",
    "\n",
    "Train on full dataset using averaged best rounds from CV, then blend with CV predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "cat_cols = train_df.drop(columns=[\"Status\", \"id\"]).select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in train_df.drop(columns=[\"Status\", \"id\"]).columns if c not in cat_cols]\n",
    "\n",
    "preprocessor = build_preprocessor(cat_cols, num_cols)\n",
    "X_all = preprocessor.fit_transform(train_df.drop(columns=[\"Status\", \"id\"]))\n",
    "X_test = preprocessor.transform(test_df.drop(columns=[\"id\"]))\n",
    "\n",
    "# Train final model\n",
    "final_params = dict(base_params)\n",
    "final_params.pop(\"early_stopping_rounds\", None)\n",
    "final_params[\"n_estimators\"] = max(avg_rounds, 50)\n",
    "\n",
    "final_model = XGBClassifier(**final_params)\n",
    "y_all = label_encoder.transform(train_df[\"Status\"])\n",
    "final_model.fit(X_all, y_all)\n",
    "\n",
    "final_test_pred = final_model.predict_proba(X_test)\n",
    "\n",
    "# Blend CV and final predictions\n",
    "blended_test_pred = (test_pred + final_test_pred) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab69103",
   "metadata": {},
   "source": [
    "## Create Submission\n",
    "\n",
    "Format predictions with proper column order (C, CL, D) and clip probabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
